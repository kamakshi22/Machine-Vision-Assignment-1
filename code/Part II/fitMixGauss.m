function mixGaussEst = fitMixGauss(data,k);

[nDim nData ] = size(data);

%MAIN E-M ROUTINE
%there are nData data points, and there is a hidden variable associated
%with each.  If the hidden variable is 0 this indicates that the data was
%generated by the first Gaussian.  If the hidden variable is 1 then this
%indicates that the hidden variable was generated by the second Gaussian
%etc.

postHidden = zeros(k, nData);

%in the E-M algorithm, we calculate a complete posterior distribution over
%the (nData) hidden variables in the E-Step.  In the M-Step, we
%update the parameters of the Gaussians (mean, cov, w).

%we will initialize the values to random values
mixGaussEst.d = nDim;
mixGaussEst.k = k;
mixGaussEst.weight = (1/k)*ones(1,k);
mixGaussEst.mean = 2*randn(nDim,k);
for (cGauss =1:k)
    mixGaussEst.cov(:,:,cGauss) = (0.5+1.5*rand(1))*eye(nDim,nDim);
end;

%calculate current likelihood
%TO DO - fill in this routine
logLike = getMixGaussLogLike(data,mixGaussEst);
fprintf('Log Likelihood Iter 0 : %4.3f\n',logLike);

likelihood=[];
nIter = 20;
for (cIter = 1:nIter)
    %Expectation step
    
    for (cData = 1:nData)
        %TO DO (g): fill in column of 'hidden' - calculate posterior probability that
        %this data point came from each of the Gaussians
        %replace this:
        thisData = data(:,cData);
        Z=0;
        for (cGauss = 1:k)
            
            likelihood(cGauss,cData)= mixGaussEst.weight(cGauss) * calcGaussianProb(thisData,mixGaussEst.mean(:,cGauss),mixGaussEst.cov(:,:,cGauss));
            Z= Z+ likelihood(cGauss,cData);
        end;
        
        postHidden(:,cData) = likelihood(:,cData)/Z;
    end;
    
    
    %Maximization Step
    
    %for each constituent Gaussian
    for (cGauss = 1:k)
        %TO DO (h):  Update weighting parameters mixGauss.weight based on the total
        %posterior probability associated with each Gaussian. Replace this:
        mixGaussEst.weight(cGauss) = sum(postHidden(cGauss,:))/  sum(sum(postHidden));
        
        %TO DO (i):  Update mean parameters mixGauss.mean by weighted average
        %where weights are given by posterior probability associated with
        %Gaussian.  Replace this:
        %          mixGaussEast.mean(:,cGauss) = (postHidden(cGauss,:)*data')/sum( postHidden(cGauss,:));
        mixGaussEst.mean(:, cGauss) = sum((postHidden(cGauss,:).*data)') / sum(postHidden(cGauss,:));
        
        
        %TO DO (j):  Update covarance parameter based on weighted average of
        %square distance from update mean, where weights are given by
        %posterior probability associated with Gaussian
        %         mixGaussEst.cov(:,:,cGauss) =  postHidden(cGauss,:)*((data - mixGaussEst.mean(:,cGauss)).^2)'/sum(postHidden(cGauss,:));
        G = zeros(nDim);
        for i=1:nData
            G = G+ (postHidden(cGauss,i).*(data(:,i) - mixGaussEst.mean(:,cGauss))*(data(:,i) - mixGaussEst.mean(:,cGauss))');
        end;
        mixGaussEst.cov(:,:,cGauss) = G/sum(postHidden(cGauss,:));
        
    end;
    
    
    
    %calculate the log likelihood
    logLike = getMixGaussLogLike(data,mixGaussEst);
    fprintf('Log Likelihood Iter %d : %4.3f\n',cIter,logLike);
    
end;


%==========================================================================
%==========================================================================

%the goal of this routine is to calculate the log likelihood for the whole
%data set under a mixture of Gaussians model. We calculate the log as the
%likelihood will probably be a very small number that Matlab may not be
%able to represent.
function logLike = getMixGaussLogLike(data,mixGaussEst);

%find total number of data items
nData = size(data,2);
%initialize log likelihoods
logLike = 0;
likelihood=[];
%run through each data item
for(cData = 1:nData)
    thisData = data(:,cData);
    %TO DO (e) - calculate likelihood of this data point under mixture of
    %Gaussians model. Replace this
    Z=0;
    for (cGauss= 1:mixGaussEst.k)
        likelihood(cGauss,cData)= mixGaussEst.weight(cGauss) * calcGaussianProb(thisData,mixGaussEst.mean(:,cGauss),mixGaussEst.cov(:,:,cGauss));
        Z= Z + likelihood(cGauss,cData);
    end;
    
    %add to total log like
    logLike = logLike+log(Z);
end;